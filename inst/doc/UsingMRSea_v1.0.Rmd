---
title: "Statistical Modelling of bird and cetacean distributions in offshore renewables development areas"
author: "LAS Scott-Hayward, C Oedekoven, CG Walker and ML Mackenzie"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
bibliography: newref.bib
---


-------

#### This vignette constitutes work carried out at the Centre for Research into Ecological and Environmental Modelling (CREEM) at the University of St. Andrews.


**Please reference this document as:**
Scott-Hayward, L.A.S., Oedekoven, C.S., Mackenzie, M.L. and Walker, C.G. (2019). Vignette for the MRSea Package v1.01: Statistical Modelling of bird and cetacean distributions in offshore renewables development areas. Centre for Research into Ecological and Environmental Modelling, University of St Andrews.

*******

```{r echo=FALSE, message=FALSE, warning=FALSE}
require(knitcitations)
cleanbib()
biblio <- read.bibtex("newref.bib")
cite_options(citation_format = 'pandoc', cite.style = 'authoryear', max.names = 1, longnamesfirst=FALSE)
knitr::opts_chunk$set(fig=TRUE, warning=FALSE, message=FALSE, eval=TRUE, comment = '')
```

## Introduction
The ```MRSea``` package was developed for analysing data that was collected for assessing potential impacts of renewable developments on marine wildlife, although the methods are applicable to other studies as well. This vignette gives an updated example of the code for version 1.01.  For additional information regarding methods, see `r citet(biblio[['Mac2013']])` and `r citet(biblio[['ScottH2013a']])`.  The user should be familiar with generalised linear models and their assumptions and model selection. The ```MRSea``` package primarily allows spatially adaptive model selection for both one and two dimensional covariates using the functions ```runSALSA1D``` and ```runSALSA2D```, which implement the methods of `r citet(biblio[['Walker2010']])` and `r citet(biblio[['ScottH2013']])`. 

The major update to this package is that a class of model `gamMRSea` is created when running either SALSA 1D or 2D. This retains within the model object infomation regarding fitting, such as the `splineParam` object and the panel structure (if present).  The use of the `summary` function on these models returns both raw and robust standard errors, with the *p*-values from the models hypothesis test using the robust standard errors.  The robust standard errors are obtained using the panel structure given (independence is one panel per data point and is the default if no structure is given).

Other functions include diagnostics (to assess residual correlation: ```runACF```, smooth relationships: ``` runPartialPlots``` and model selection (ANOVA) for robust standard errros: ```anova.gamMRSea```) and inference (```do.bootstrap.cress.robust```). 


![alt text](MRSea_workflow.png)
Example of the modelling process using MRSea.  Packages with functions to run certain parts are given in oval boxes.  To complete the modelling process, other packages may be used at certain stages.  These are coded light blue, whilst MRSea functions are in red.  


A full description of each of the functions within the ```MRSea``` package can be found in the reference manual at: 
http://creem2.st-and.ac.uk/software.aspx.  The manual and this document use version 0.2.2 of MRSea.

## Distance sampling using the ```mrds``` library

1. Load data and fit detection function (Distance Sampling)
```{r message=FALSE}
devtools::load_all(path = '../../MRSea')
# we will use the dataset with a known re-distribution of animals
data(dis.data.re)
dis.data<-dis.data.re
require(mrds) # distance sampling package
result <- ddf(dsmodel=~mcds(key="hn", formula=~1),
              data = dis.data, method="ds", 
              meta.data=list(width=250))
```

2. Adjust sightings for detectability 
```{r dist, cache=TRUE, results='hide', warning=FALSE, message=FALSE}
# create.NHAT and create.count.data are MRSea functions to adjust the 
# sightings for the detection function estimated above.
dis.data <- create.NHAT(dis.data,result)
count.data <- create.count.data(dis.data)
```

3. Try a simple model
```{r message=FALSE, warning=FALSE}
data <- count.data
data$response <- round(data$NHAT)
attach(data)
fullModel <- glm(response ~ as.factor(season) + as.factor(impact) +
                   depth + x.pos + y.pos, family = poisson, data = data)
```

4.  Try a model with a smooth term for depth
```{r message=FALSE}
require(splines)
fullModel <- glm(response ~ as.factor(season) + as.factor(impact) +
                   bs(depth, knots = mean(depth)) + x.pos + y.pos, 
                 family = poisson,data = data)
```

5.  If the data are correlated then you may wish to specify a blocking structure in your dataset. 


For correlated data:
```{r }
data$blockid <- paste(data$transect.id, data$season, data$impact,sep = "")
```


## Selection of 1D Covariates
Run SALSA1D to select what covariates are included and whether or not they are smooth.  SALSA selects the smoothness of each term (number and location of knots) and 10-fold CV is used to choose between the best smooth term, a linear term or no term at all.  To not allow the removal process the user may set ```removal = FALSE``` as a parameter in the function ```runSALSA1D```.

6. Specify the parameters required:

```{r }
salsa1dlist <- list(fitnessMeasure = "cv.gamMRSea", minKnots_1d = 2,maxKnots_1d = 5, 
                    startKnots_1d = 1, degree = 2, maxIterations = 10,
                    gaps = c(0), cv.opts=list(cv.gamMRSea.seed=1, K=10))
```

7.  If you wish to make predictions once the model is fitted, then a prediction grid should be created and specified.  This is because the splines fitted here (B-splines) are unable to make predictions outside of the range they were created.  For example, if the data range for depth is smaller than the range of depths in the prediction data, predictions cannot be made.  

```{r }
data("nysted.predictdata")  # contains predict.data
# This is a spatial grid for making predictions.  All covariates in 
# final model must be in this data frame and the naming must be the 
# same as for the data
predictData <- nysted.predictdata
range(data$depth)
range(predictData$depth)
```

Here the range of the predictions is slightly wider than the range of the data, so we will specify ```predictData``` when running SALSA.

8. Set up the initial model with factor covariates and the offset term (if required), and run SALSA.
```{r message=FALSE}
initialModel <- glm(response ~ as.factor(season) + as.factor(impact) 
                    + offset(log(area)), family = "quasipoisson", 
                    data = data)
```

```{r message=FALSE, warning=FALSE, echo=FALSE, results='hide'}
# run SALSA
require(MuMIn)
salsa1dOutput <- runSALSA1D(initialModel, salsa1dlist, c("depth"),
                      predictionData=predictData, datain=data, removal=TRUE, panelid = data$blockid)
```

```{r eval=FALSE}
# run SALSA
salsa1dOutput <- runSALSA1D(initialModel, salsa1dlist, c("depth"),
                      predictionData=predictData, datain=data, removal=TRUE, panelid = data$blockid)
```

Use the built in summary function (`summary.gamMRSea`) to look at the summary of the model.  Note that robust standard errors are given alongside the raw standard erorrs and information regarding panels is at the bottom of the output. If each data point is a panel, then independence is assumed.

```{r }
summary(salsa1dOutput$bestModel)
```


```{r }
# How many knots were chosen for depth?
salsa1dOutput$splineParams[[2]]$knots
# ~~~~~~~~~~~~~~~~~~~~~~~
```


## Selection of flexibility for 2D smooth term

9. Create a grid of knots that will be used as possible knot locations.  This may take while and could be different every time you run it so I suggest saving the knotgrid as a file.

```{r knotgrid, message=FALSE, fig=TRUE, fig.align='center', fig.width=9, fig.height=6, cache=TRUE}
knotgrid<- getKnotgrid(coordData = cbind(data$x.pos, data$y.pos), numKnots = 300)
#
# write.csv(knotgrid, file='knotgrid_fullanalysis.csv', row.names=F)
# ~~~~~~~~~~~~~~~~~~~~~~~
```

The black points in the figure are the data and the red points, the candidate knot locations. By default, the knotgrid has 300 knot positions chosen.

10. Set up parameters for SALSA2D.  Distance matrices (data to knots and knot to knots), a fit statistic and min, max and start knots.

```{r }
# make distance matrices for datatoknots and knottoknots
distMats <- makeDists(cbind(data$x.pos, data$y.pos), knotgrid)

# ~~~~~~~~~~~~~~~~~~~~~~~

# make parameter set for running salsa2d
salsa2dlist<-list(fitnessMeasure = 'cv.gamMRSea', knotgrid = knotgrid, 
                 startKnots=5, minKnots=4, maxKnots=12, gap=0, 
                 interactionTerm="as.factor(impact)", cv.opts=list(cv.gamMRSea.seed=1, K=10))
```

11. Run SALSA2D to find the appropriate number and location of knots for the 2D smooth term of `x.pos` and `y.pos`.
```{r echo=FALSE, message=FALSE, warning=FALSE, results='hide'}
salsa2dOutput<-runSALSA2D(salsa1dOutput$bestModel, salsa2dlist, 
                      d2k=distMats$dataDist,k2k=distMats$knotDist)
```

```{r echo=TRUE, eval=FALSE}
salsa2dOutput<-runSALSA2D(salsa1dOutput$bestModel, salsa2dlist, 
                             d2k=distMats$dataDist, k2k=distMats$knotDist)
```

```{r }
plot(data$x.pos, data$y.pos, col="grey", pch=16,
    xlab="X", ylab="Y", asp=1)
points(knotgrid, pch=16, col=4)
points(knotgrid[salsa2dOutput$aR[[1]],], 
       col="darkgreen", pch=16, cex=2)  
```

12.  Are the residuals correlated? Make a suitable blocking structure, within which residuals are expected to be correlated but between which they are independent.  Use ```runACF``` to assess the blocking structure.

```{r acfplot, fig.cap='ACF plot showing correlation in each block (grey lines), and the mean correlation by lag across blocks (red line).'}
runACF(block = data$blockid, model = salsa2dOutput$bestModel, suppress.printout=TRUE)
```

Here we also do a runs test to assess for correlation in the model residuals.  Since our data are over-dispersed, we must use the empirical distribution for assessment:

```{r}
simData<-generateNoise(n=500, response=fitted(salsa2dOutput$bestModel), family='poisson', d=summary(salsa2dOutput$bestModel)$dispersion)
empdist<-getEmpDistribution(500, simData, salsa2dOutput$bestModel, data=data,dots=FALSE)
runsTest(residuals(salsa2dOutput$bestModel, type='pearson'),emp.distribution=empdist)
```


13. Check for model selection
```{r }
set.seed(1)
cv.gamMRSea(data=data, modelobject = salsa2dOutput$bestModel, K=10)$delta[2]
salsa2dOutput$fitStat
```

```{r}
anova(salsa2dOutput$bestModel)
```

```{r fig=TRUE, fig.align='center', fig.width=6, fig.height=4, message=FALSE}
par(mfrow=c(2,2))
runPartialPlots(model = salsa2dOutput$bestModel, data = data, factorlist.in = 
                  c('season', 'impact'), varlist.in = 'depth', showKnots = T)
```

```{r fig=TRUE, fig.align='center', fig.width=6, fig.height=4, message=FALSE}
par(mfrow=c(2,2))
runPartialPlots(model = salsa2dOutput$bestModel, data = data, factorlist = 
                  c('season', 'impact'), varlist = 'depth', showKnots = T, type='link')
```

## Making Predictions

```{r }
dists<-makeDists(cbind(predictData$x.pos, predictData$y.pos), 
                 knotgrid,knotmat=FALSE)$dataDist


# make predictions on response scale
preds<-predict.gamMRSea(newdata = predictData, g2k = dists, object = salsa2dOutput$bestModel)
```

Plotting the predictions pre and post impact:
```{r fig=TRUE, fig.align='center', fig.width=9, fig.height=6}
par(mfrow=c(1,2))
quilt.plot(predictData$x.pos[predictData$impact==0], 
           predictData$y.pos[predictData$impact==0], 
           preds[predictData$impact==0], nrow=104, ncol=55, asp=1)

quilt.plot(predictData$x.pos[predictData$impact==1], 
           predictData$y.pos[predictData$impact==1], 
           preds[predictData$impact==1], nrow=104, ncol=55, asp=1)
```

## Bootstrapped Confidence Intervals and Difference Surfaces

14. The coding in this section has changed slightly from the original user guide.
15. Bootstrap to include parameter estimation uncertainty in the detection function and parameter estimation in the spatial model. (Note: If no detection function estimated, then the bootstrap is just on the parameters of the spatial model.)

```{r boots, warning=FALSE, message=FALSE, results='hide'}
dis.data$seasonimpact <- paste(dis.data$season, dis.data$impact)

bootPreds<-do.bootstrap.cress.robust(salsa2dOutput$bestModel, predictionGrid = predictData, result, splineParams=salsa2dOutput$bestModel$splineParams, g2k=dists, B = 100, robust=TRUE)
```

```{r }
#load('predictionboot.RData')
cis <- makeBootCIs(bootPreds)
```

16. Calculate the differences before and after across all bootstraps
```{r }
differences <- getDifferences(beforePreds = 
                      bootPreds[predictData$impact == 0, ],
                      afterPreds = bootPreds[predictData$impact == 1, ])
```

17. Plot differences and indicate where significant positive/negative differences lie.
```{r fig=TRUE, fig.align='center', fig.width=9, fig.height=6}
mediandiff <- differences$mediandiff
# The marker for each after - before difference:
# positive ('1') and negative ('-') significant differences
marker <- differences$significanceMarker
par(mfrow = c(1, 1))
quilt.plot(predictData$x.pos[predictData$impact == 0], 
           predictData$y.pos[predictData$impact == 0],
           mediandiff, asp = 1, nrow = 104, ncol = 55)
# add + or - depending on significance of cells. Just
# requires one significance out of all to be allocated
points(predictData$x.pos[predictData$impact == 0][marker == 1],
       predictData$y.pos[predictData$impact == 0][marker == 1],
       pch = "+", col = "darkgrey", cex = 0.75)
points(predictData$x.pos[predictData$impact == 0][marker == (-1)],
       predictData$y.pos[predictData$impact == 0][marker == (-1)],
       col = "darkgrey", cex = 0.75)
points(681417.3/1000, 6046910/1000, cex = 3, pch = "*", lwd = 1, col = "grey")
```

